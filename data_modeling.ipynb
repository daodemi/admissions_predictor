{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test.csv')\n",
    "df = df.drop(['Unnamed: 0'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new feature - number of chronic diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_chronic_diseases(row):\n",
    "    num = 0\n",
    "    chron_column = ['CHRON1', 'CHRON2', 'CHRON3', 'CHRON4', 'CHRON5','CHRON6', 'CHRON7', 'CHRON8', 'CHRON9', 'CHRON10', 'CHRON11', 'CHRON12','CHRON13', 'CHRON14', 'CHRON15']\n",
    "    for column in chron_column:\n",
    "        if row[column] == 1:\n",
    "            num += 1\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chronic_diseases(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NUM_CHRON'] = df.apply(num_chronic_diseases, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['CHRON1', 'CHRON2', 'CHRON3', 'CHRON4', 'CHRON5','CHRON6', 'CHRON7', 'CHRON8', 'CHRON9', 'CHRON10', 'CHRON11', 'CHRON12','CHRON13', 'CHRON14', 'CHRON15'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.NUM_CHRON.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new feature - adding buckets for diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"INFECTIOUS_PARASITIC\"] = 0\n",
    "df[\"NEOPLASMS\"] = 0\n",
    "df[\"ENDOCRINE\"] = 0\n",
    "df[\"BLOOD\"] = 0\n",
    "df[\"MENTAL_DISORDER\"] = 0\n",
    "df[\"NERVOUS_SYSTEM\"] = 0\n",
    "df[\"CIRCULATORY_SYSTEM\"] = 0\n",
    "df[\"RESPIRATORY\"] = 0\n",
    "df[\"DISGESTIVE\"] = 0\n",
    "df[\"GENITOURINARY\"] = 0\n",
    "df[\"PREGNANCY_COMPLICATION\"] = 0\n",
    "df[\"SKIN\"] = 0\n",
    "df[\"MUSCULOSKELETAL\"] = 0\n",
    "df[\"CONGENITAL_ANOMALIES\"] = 0\n",
    "df[\"PERINATAL\"] = 0\n",
    "df[\"ILL-DEFINED\"] = 0\n",
    "df[\"INJURY_AND_POISONING\"] = 0\n",
    "df[\"EXTERNAL_CAUSES\"] = 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_bucket(row, lower, upper):\n",
    "    num = 0\n",
    "    diagnoses = ['DX1', 'DX2', 'DX3', 'DX4', 'DX5', 'DX6', 'DX7', 'DX8', 'DX9', 'DX10', 'DX11', 'DX12', 'DX13', 'DX14','DX15']\n",
    "    for diagnosis in diagnoses:\n",
    "        dx = str(row[diagnosis])\n",
    "        if dx == 'nan': \n",
    "            x = 0\n",
    "        elif (dx[0] == 'V') or (dx[0] == 'E'):\n",
    "            x = 0\n",
    "        else:\n",
    "            try:\n",
    "                x = int(dx)\n",
    "            except: \n",
    "                x = 0\n",
    "\n",
    "        if lower < x < upper:\n",
    "            num += 1\n",
    "            \n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df.iloc[5193754]\n",
    "\n",
    "num = 0\n",
    "diagnoses = ['DX1', 'DX2', 'DX3', 'DX4', 'DX5', 'DX6', 'DX7', 'DX8', 'DX9', 'DX10', 'DX11', 'DX12', 'DX13', 'DX14','DX15']\n",
    "for diagnosis in diagnoses:\n",
    "        dx = str(row[diagnosis])\n",
    "        if dx == 'nan': \n",
    "            x = 0\n",
    "        elif (dx[0] == 'V') or (dx[0] == 'E'):\n",
    "            x = 0\n",
    "        else:\n",
    "            try:\n",
    "                x = int(dx)\n",
    "            except: \n",
    "                x = 0\n",
    "\n",
    "        if 100 < x < 4000:\n",
    "            num += 1\n",
    "\n",
    "num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['INFECTIOUS_PARASITIC'] = df.apply((lambda x: count_bucket(x, 100, 13900)), axis=1)\n",
    "df[\"NEOPLASMS\"] = df.apply((lambda x: count_bucket(x, 14000, 23900)), axis=1)\n",
    "df[\"ENDOCRINE\"] = df.apply((lambda x: count_bucket(x, 24000, 27900)), axis=1)\n",
    "df[\"BLOOD\"] = df.apply((lambda x: count_bucket(x, 28000, 28900)), axis=1)\n",
    "df[\"MENTAL_DISORDER\"] = df.apply((lambda x: count_bucket(x, 29000, 31900)), axis=1)\n",
    "df[\"NERVOUS_SYSTEM\"] = df.apply((lambda x: count_bucket(x, 32000, 38900)), axis=1)\n",
    "df[\"CIRCULATORY_SYSTEM\"] = df.apply((lambda x: count_bucket(x, 39000, 45900)), axis=1)\n",
    "df[\"RESPIRATORY\"] = df.apply((lambda x: count_bucket(x, 46000 , 51900)), axis=1)\n",
    "df[\"DISGESTIVE\"] = df.apply((lambda x: count_bucket(x, 52000 , 57900)), axis=1)\n",
    "df[\"GENITOURINARY\"] = df.apply((lambda x: count_bucket(x, 58000, 62900)), axis=1)\n",
    "df[\"PREGNANCY_COMPLICATION\"] = df.apply((lambda x: count_bucket(x, 63000, 67900)), axis=1)\n",
    "df[\"SKIN\"] = df.apply((lambda x: count_bucket(x, 68000, 70900)), axis=1)\n",
    "df[\"MUSCULOSKELETAL\"] = df.apply((lambda x: count_bucket(x, 71000, 73900)), axis=1)\n",
    "df[\"CONGENITAL_ANOMALIES\"] = df.apply((lambda x: count_bucket(x, 74000, 75900)), axis=1)\n",
    "df[\"PERINATAL\"] = df.apply((lambda x: count_bucket(x, 76000, 77900)), axis=1)\n",
    "df[\"ILL-DEFINED\"] = df.apply((lambda x: count_bucket(x, 78000, 79900)), axis=1)\n",
    "df[\"INJURY_AND_POISONING\"] = df.apply((lambda x: count_bucket(x, 80000 , 99900 )), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_external_bucket(row):\n",
    "    num = 0\n",
    "    diagnoses = ['DX1', 'DX2', 'DX3', 'DX4', 'DX5', 'DX6', 'DX7', 'DX8', 'DX9', 'DX10', 'DX11', 'DX12', 'DX13', 'DX14','DX15']\n",
    "    for diagnosis in diagnoses:\n",
    "        try:\n",
    "            if (str(row[diagnosis][0])) == 'V' or (str(row[diagnosis][0]) == 'E'):\n",
    "                num += 1\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"EXTERNAL_CAUSES\"] = df.apply(count_external_bucket, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['DX1', 'DX2', 'DX3', 'DX4', 'DX5',\n",
    "       'DX6', 'DX7', 'DX8', 'DX9', 'DX10', 'DX11', 'DX12', 'DX13', 'DX14',\n",
    "       'DX15', 'PAY1'],1)\n",
    "# df.to_csv('sample1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting dummies for urban/rural scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_rural = pd.get_dummies(df['PL_NCHS2006'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_rural = urban_rural.drop(-99,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_rural.columns = ['LARGE_CENTRAL_METRO', 'LARGE_FRINGE_METRO', 'MEDIUM_METRO', 'SMALL_METRO', 'MICROPOLITAN', 'NONPOLITAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_rural.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_rural.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(urban_rural)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['PL_NCHS2006'], 1)\n",
    "df.to_csv('sample1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sample2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample2.csv')\n",
    "df = df.drop(['Unnamed: 0','KEY_ED','PL_NCHS2006','ZIPINC_QRTL'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['TOTCHG_ED'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['EDEVENT']==2) | (df['EDEVENT']==1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in list(df.EDEVENT.unique()):\n",
    "    print(event, ': ', (list(df.EDEVENT).count(event)/len(list(df.EDEVENT)) * 100),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import precision_score, recall_score, roc_curve, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, \n",
    "                              AdaBoostClassifier, BaggingRegressor)\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into training, validation, and holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('EDEVENT', axis=1)\n",
    "y = df['EDEVENT']\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=.25, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "nb = GaussianNB()\n",
    "logreg = LogisticRegression()\n",
    "rf = RandomForestClassifier()\n",
    "gb = GradientBoostingClassifier()\n",
    "xgb = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_fit = knn.fit(X_train,y_train)\n",
    "nb_fit = nb.fit(X_train,y_train)\n",
    "lr_fit = logreg.fit(X_train,y_train)\n",
    "rf_fit = rf.fit(X_train,y_train)\n",
    "gb_fit = gb.fit(X_train,y_train)\n",
    "xgb_fit = xgb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fits = [knn_fit, nb_fit, lr_fit, rf_fit, gb_fit]\n",
    "model_names = ['knn','naive bayes','logistic regression','random forest', 'gradient boosted']\n",
    "\n",
    "model_list = list(zip(model_names, model_fits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_voting_classifier = VotingClassifier(estimators=model_list,\n",
    "                                    voting='hard', \n",
    "                                    n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_voting_classifier = VotingClassifier(estimators=model_list,\n",
    "                                    voting='soft', \n",
    "                                    n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = StackingClassifier(\n",
    "    classifiers=model_fits, meta_classifier=BernoulliNB(), use_probas=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average accuracy across 3 cross-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"knn: \", sum(cross_val_score(knn, X_train, y_train, cv=3, n_jobs=-1, scoring='accuracy'))/3)\n",
    "print(\"nb: \", sum(cross_val_score(nb, X_train, y_train, cv=3, n_jobs=-1, scoring='accuracy'))/3)\n",
    "print(\"logistic regression: \", sum(cross_val_score(logreg, X_train, y_train, cv=3, n_jobs=-1, scoring='accuracy'))/3)\n",
    "print(\"random forest: \", sum(cross_val_score(rf, X_train, y_train, cv=3, n_jobs=-1, scoring='accuracy'))/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average recall across 3 cross-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"knn: \", sum(cross_val_score(knn, X_train, y_train, cv=3, n_jobs=-1, scoring='recall'))/3)\n",
    "print(\"nb: \", sum(cross_val_score(nb, X_train, y_train, cv=3, n_jobs=-1, scoring='recall'))/3)\n",
    "print(\"logistic regression: \", sum(cross_val_score(logreg, X_train, y_train, cv=3, n_jobs=-1, scoring='recall'))/3)\n",
    "print(\"random forest: \", sum(cross_val_score(rf, X_train, y_train, cv=3, n_jobs=-1, scoring='recall'))/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average precision across 3 cross-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"knn: \", sum(cross_val_score(knn, X_train, y_train, cv=3, n_jobs=-1, scoring='precision'))/3)\n",
    "print(\"nb: \", sum(cross_val_score(nb, X_train, y_train, cv=3, n_jobs=-1, scoring='precision'))/3)\n",
    "print(\"logistic regression: \", sum(cross_val_score(logreg, X_train, y_train, cv=3, n_jobs=-1, scoring='precision'))/3)\n",
    "print(\"random forest: \", sum(cross_val_score(rf, X_train, y_train, cv=3, n_jobs=-1, scoring='precision'))/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average F1 across 3 cross-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"knn: \", sum(cross_val_score(knn, X_train, y_train, cv=3, n_jobs=-1, scoring='f1_micro'))/3)\n",
    "print(\"nb: \", sum(cross_val_score(nb, X_train, y_train, cv=3, n_jobs=-1, scoring='f1_micro'))/3)\n",
    "print(\"logistic regression: \", sum(cross_val_score(logreg, X_train, y_train, cv=3, n_jobs=-1, scoring='f1_micro'))/3)\n",
    "print(\"random forest: \", sum(cross_val_score(rf, X_train, y_train, cv=3, n_jobs=-1, scoring='f1_micro'))/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"gradient boost: \", sum(cross_val_score(gb, X_train, y_train, cv=3, n_jobs=-1, scoring='f1_micro'))/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"XGBoost: \", sum(cross_val_score(xgb, X_train, y_train, cv=3, n_jobs=-1, scoring='f1_micro'))/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max Voting: \", sum(cross_val_score(max_voting_classifier, X_train, y_train, cv=3, n_jobs=-1, scoring='f1_micro'))/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Avg Voting: \", sum(cross_val_score(avg_voting_classifier, X_train, y_train, cv=3, n_jobs=-1, scoring='f1_micro'))/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stacked Esemble: \", sum(cross_val_score(stacked, X_train, y_train, cv=3, n_jobs=-1, scoring='f1_micro'))/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing models on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pred = knn.fit(X_train,y_train).predict(X_val)\n",
    "print('knn: ',f1_score(y_val, knn_pred) )\n",
    "      \n",
    "nb_pred = nb.fit(X_train,y_train).predict(X_val)\n",
    "print('nb: ',f1_score(y_val, nb_pred))\n",
    "      \n",
    "logreg_pred = logreg.fit(X_train,y_train).predict(X_val)\n",
    "print('logistic regression: ',f1_score(y_val, logreg_pred)) \n",
    "\n",
    "rf_pred = rf.fit(X_train,y_train).predict(X_val)\n",
    "print('random forest: ',f1_score(y_val, rf_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_pred = max_voting_classifier.fit(X_train, y_train).predict(X_val)\n",
    "print('Max Voting: ',f1_score(y_val, mv_pred)) \n",
    "\n",
    "avg_pred = avg_voting_classifier.fit(X_train, y_train).predict(X_val)\n",
    "print('Avg Voting: ',f1_score(y_val, avg_pred)) \n",
    "\n",
    "gb_pred = gb.fit(X_train, y_train).predict(X_val)\n",
    "print('Gradient Boosted: ',f1_score(y_val, gb_pred)) \n",
    "\n",
    "stacked_pred = stacked.fit(X_train, y_train).predict(X_val)\n",
    "print('Stacked Ensemble: ',f1_score(y_val, stacked_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pred = xgb.fit(X_train, y_train).predict(X_val)\n",
    "print('XGBoost: ',f1_score(y_val, xgb_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing random forest on the holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_test_pred = xgb.fit(X_train,y_train).predict(X_test)\n",
    "print('XG Boost on holdout set: ',f1_score(y_test, xgb_test_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = list(zip(X.columns,xgb.feature_importances_))\n",
    "importances.sort(key = lambda t: t[1])\n",
    "importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting importance thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df.iloc[20]\n",
    "row = row.drop(labels=['EDEVENT'])\n",
    "chron = list(range(0,16))\n",
    "age = list(range(0,91))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_list = row.axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.setdiff1d(df_list,row_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = xgb_fit.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df = pd.DataFrame(columns=['NUM_CHRON','AGE'])\n",
    "prob_df['NUM_CHRON'] = X_test['NUM_CHRON']\n",
    "prob_df['AGE'] = X_test['AGE']\n",
    "prob_df['PROB_RELEASED'] = prob[:,0] \n",
    "prob_df['PROB_ADMITTED'] = prob[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_chron = prob_df.drop('AGE',1)\n",
    "prob_age = prob_df.drop('NUM_CHRON',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df.to_csv('prob.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Span\n",
    "\n",
    "output_file(\"chron.html\")\n",
    "\n",
    "p = figure(title=\"Avg Probability of Admittance vs Number of Chronic Diseases\",plot_width=1000, plot_height=600)\n",
    "\n",
    "# add a line renderer\n",
    "p.line(chron.NUM_CHRON, chron.PROB_ADMITTED, line_width=3)\n",
    "\n",
    "# Horizontal line\n",
    "hline = Span(location=0.5, dimension='width', line_color='red', line_width=2, line_dash='dotted')\n",
    "p.renderers.extend([hline])\n",
    "\n",
    "p.xaxis.axis_label_text_font_size = \"15pt\"\n",
    "p.yaxis.axis_label_text_font_size = \"15pt\"\n",
    "p.xaxis.axis_label=\"Number of Chronic Diseases\"\n",
    "p.yaxis.axis_label=\"Average Admittance Probability\"\n",
    "p.title.text_font_size = \"20pt\"\n",
    "\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob_chron.groupby(['NUM_CHRON'], as_index=False).mean().to_csv('chron.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = prob_age.groupby([\"AGE\"], as_index=False).mean()\n",
    "age.to_csv('age.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chron = prob_chron.groupby([\"NUM_CHRON\"], as_index=False).mean()\n",
    "chron.to_csv('chron.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating risk ratio heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital = pd.read_csv('NEDS_2012_HOSPITAL.csv')\n",
    "hospital.columns = ['DISCWT','HOSPWT','HOSP_CONTROL','HOSP_ED','HOSP_REGION','HOSP_TRAUMA','HOSP_URCAT','HOSP_URTEACH','NEDS_STRATUM','N_DISC','N_HOSP','S_DISC','S_HOSP','TOTAL_ED','YEAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpatient = pd.read_csv('NEDS_2012_IP.csv')\n",
    "inpatient.columns=['HOSP_ED','KEY_ED','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32','34','34','35','36','TOT_IP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[test['EDEVENT'] == 2]\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[['HOSP_ED','ZIPINC_QRTL', 'KEY_ED']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_dict = hospital.groupby('HOSP_ED')['HOSP_REGION'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['REGION'] = test['HOSP_ED'].map(region_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.dropna(axis=0, subset=['REGION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_dict = inpatient.groupby('KEY_ED')['TOT_IP'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['COST'] = test['KEY_ED'].map(cost_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['REGION'] = test['REGION'].map(lambda x: x[0])\n",
    "test['COST'] = test['COST'].map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[test['COST'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['ZIPINC_QRTL'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[test['ZIPINC_QRTL'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_dict = {\n",
    "    1 : '38999',\n",
    "    2 : '47999',\n",
    "    3 : '62999',\n",
    "    4 : '72000'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['MEDIAN_INCOME'] = test['ZIPINC_QRTL'].map(median_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['MEDIAN_INCOME'] = pd.to_numeric(test['MEDIAN_INCOME'])\n",
    "test['COST'] = pd.to_numeric(test['COST'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['RISK_RATIO'] = test['COST'] / test['MEDIAN_INCOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(test[test['REGION'] == 4]['RISK_RATIO'],bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data = test.groupby(['REGION'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data.drop(['HOSP_ED','ZIPINC_QRTL','KEY_ED'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(test[test['REGION'] == 4]['COST'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "medians = []\n",
    "\n",
    "for i in range(1,5):\n",
    "    medians.append(statistics.median(test[test['REGION'] == i]['COST']))\n",
    "\n",
    "medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = test.groupby(['REGION'], as_index=False).mean()\n",
    "heatmap = heatmap[['REGION','RISK_RATIO']]\n",
    "heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap.REGION = ['Northeast','Midwest','South','West']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\"Alabama\",\"Alaska\",\"Arizona\",\"Arkansas\",\"California\",\"Colorado\",\n",
    "  \"Connecticut\",\"Delaware\",\"Florida\",\"Georgia\",\"Hawaii\",\"Idaho\",\"Illinois\",\n",
    "  \"Indiana\",\"Iowa\",\"Kansas\",\"Kentucky\",\"Louisiana\",\"Maine\",\"Maryland\",\n",
    "  \"Massachusetts\",\"Michigan\",\"Minnesota\",\"Mississippi\",\"Missouri\",\"Montana\",\n",
    "  \"Nebraska\",\"Nevada\",\"New Hampshire\",\"New Jersey\",\"New Mexico\",\"New York\",\n",
    "  \"North Carolina\",\"North Dakota\",\"Ohio\",\"Oklahoma\",\"Oregon\",\"Pennsylvania\",\n",
    "  \"Rhode Island\",\"South Carolina\",\"South Dakota\",\"Tennessee\",\"Texas\",\"Utah\",\n",
    "  \"Vermont\",\"Virginia\",\"Washington\",\"West Virginia\",\"Wisconsin\",\"Wyoming\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = pd.DataFrame(columns=['STATE','RISK_RATIO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap['STATE'] = states\n",
    "heatmap['RISK_RATIO'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap.loc[heatmap.STATE == 'California', 'RISK_RATIO'] = 1.068500\n",
    "heatmap.loc[heatmap.STATE == 'Nevada', 'RISK_RATIO'] = 1.068500\n",
    "heatmap.loc[heatmap.STATE == 'Utah', 'RISK_RATIO'] = 1.068500\n",
    "heatmap.loc[heatmap.STATE == 'Arizona', 'RISK_RATIO'] = 1.068500\n",
    "heatmap.loc[heatmap.STATE == 'Hawaii', 'RISK_RATIO'] = 1.068500\n",
    "\n",
    "heatmap.loc[heatmap.STATE == 'North Dakota', 'RISK_RATIO'] = 0.617648\n",
    "heatmap.loc[heatmap.STATE == 'South Dakota', 'RISK_RATIO'] = 0.617648\n",
    "heatmap.loc[heatmap.STATE == 'Nebraska', 'RISK_RATIO'] = 0.617648\n",
    "heatmap.loc[heatmap.STATE == 'Kansas', 'RISK_RATIO'] = 0.617648\n",
    "heatmap.loc[heatmap.STATE == 'Missouri', 'RISK_RATIO'] = 0.617648\n",
    "heatmap.loc[heatmap.STATE == 'Iowa', 'RISK_RATIO'] = 0.617648\n",
    "heatmap.loc[heatmap.STATE == 'Minnesota', 'RISK_RATIO'] = 0.617648\n",
    "heatmap.loc[heatmap.STATE == 'Wisconsin', 'RISK_RATIO'] = 0.617648\n",
    "heatmap.loc[heatmap.STATE == 'Illinois', 'RISK_RATIO'] = 0.617648\n",
    "heatmap.loc[heatmap.STATE == 'Indiana', 'RISK_RATIO'] = 0.617648\n",
    "heatmap.loc[heatmap.STATE == 'Ohio', 'RISK_RATIO'] = 0.617648\n",
    "\n",
    "heatmap.loc[heatmap.STATE == 'Florida', 'RISK_RATIO'] = 0.783821\n",
    "heatmap.loc[heatmap.STATE == 'Georgia', 'RISK_RATIO'] = 0.783821\n",
    "heatmap.loc[heatmap.STATE == 'Tennessee', 'RISK_RATIO'] = 0.783821\n",
    "heatmap.loc[heatmap.STATE == 'Kentucky', 'RISK_RATIO'] = 0.783821\n",
    "heatmap.loc[heatmap.STATE == 'South Carolina', 'RISK_RATIO'] = 0.783821\n",
    "heatmap.loc[heatmap.STATE == 'North Carolina', 'RISK_RATIO'] = 0.783821\n",
    "\n",
    "heatmap.loc[heatmap.STATE == 'New York', 'RISK_RATIO'] = 0.636009\n",
    "heatmap.loc[heatmap.STATE == 'New Jersey', 'RISK_RATIO'] = 0.636009\n",
    "heatmap.loc[heatmap.STATE == 'Connecticut', 'RISK_RATIO'] = 0.636009\n",
    "heatmap.loc[heatmap.STATE == 'Maryland', 'RISK_RATIO'] = 0.636009\n",
    "heatmap.loc[heatmap.STATE == 'Rhode Island', 'RISK_RATIO'] = 0.636009\n",
    "heatmap.loc[heatmap.STATE == 'Maine', 'RISK_RATIO'] = 0.636009\n",
    "heatmap.loc[heatmap.STATE == 'New Hampshire', 'RISK_RATIO'] = 0.636009\n",
    "heatmap.loc[heatmap.STATE == 'Delaware', 'RISK_RATIO'] = 0.636009\n",
    "heatmap.loc[heatmap.STATE == 'Massachusetts', 'RISK_RATIO'] = 0.636009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = pd.read_csv('heatmap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abr = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\",\n",
    "\"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\",\n",
    "\"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\",\n",
    "\"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\",\n",
    "\"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(abr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap['ABR'] = abr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap.to_csv('heatmap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
